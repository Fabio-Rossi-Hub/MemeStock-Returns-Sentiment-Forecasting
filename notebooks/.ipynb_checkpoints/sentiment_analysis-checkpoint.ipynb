{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c9d24a1",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with VADER\n",
    "Since our text data is not labeled, and this project was created before the advent of LLMs, we will use VADER to perform the sentiment analysis.\n",
    "\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a pre-trained sentiment analysis model designed for analyzing sentiments in social media texts. It assigns sentiment scores to words and combines them to calculate the overall sentiment of a text, taking into account context, intensity, and grammatical structure.\n",
    "\n",
    "But as we saw previously Wallstretbets has its own slang, so we will need to add a few words and change some others. To see how I updated the library please go to vader/vader_mod.r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe366be",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#Load modified VADER\n",
    "library(vader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc87df5a",
   "metadata": {},
   "source": [
    "Extracting the sentiment is pretty straightforward, we will just calculate sentiment for each post and map it to all the stocks mentioned inside the post. This is because most of the posts don't try to articulate a reasoning, like in the DD section, but they usually express a simple opinion like \"TO THE MOON GME BB NOK\", to share a significant loss or gain \"YOLOED my life savings on AMC\", or just to make some memes. For this reason we will simplify the process and assign the same sentiment to all ticker mentioned in a post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23dbe170",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in file(file, \"rt\"):\n",
      "\"non è possibile aprire il file 'data/stock_mentions.csv': No such file or directory\"\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in file(file, \"rt\"): non è possibile aprire la connessione\n",
     "output_type": "error",
     "traceback": [
      "Error in file(file, \"rt\"): non è possibile aprire la connessione\nTraceback:\n",
      "1. read.csv(file = \"data/stock_mentions.csv\", header = TRUE, sep = \",\", \n .     dec = \".\")",
      "2. read.table(file = file, header = header, sep = sep, quote = quote, \n .     dec = dec, fill = fill, comment.char = comment.char, ...)",
      "3. file(file, \"rt\")"
     ]
    }
   ],
   "source": [
    "# Open ticker labeled reddit posts \n",
    "reddit_posts <- read.csv(file = \"data/stock_mentions.csv\",\n",
    "                           header = TRUE,\n",
    "                           sep = \",\",\n",
    "                           dec = \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8c290b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    " vader <- reddit_posts%\n",
    "   select(text) %>%\n",
    "   distinct() %>%\n",
    "  mutate(\n",
    "    comment_clean = str_replace_all(text, \"\\\\\\\\\", \" \")) %>%\n",
    "   mutate(sentiment = vader_df(comment_clean)$compound)\n",
    " \n",
    "\n",
    "\n",
    "reddit_sentiment <- redreddit_posts\n",
    "   left_join(vader %>% select(-comment_clean),\n",
    "              by = \"text\") %>%\n",
    "            select(-X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045eee5a",
   "metadata": {},
   "source": [
    "PS. I have some graphs in another document, I'll add them in another commit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
